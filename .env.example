# Stoma Environment Configuration Template
# Copy this file to .env and fill in your actual values

# === Deep Research Integration Configuration ===
ENABLE_DEEP_RESEARCH=true

# Model Configuration - Local Ollama Models (Recommended)
# Options: ollama:llama3.1:latest, ollama:phi3.5:latest, ollama:qwen2.5-coder:3b
DR_RESEARCH_MODEL=ollama:llama3.1:latest
DR_FINAL_REPORT_MODEL=ollama:llama3.1:latest
DR_SUMMARIZATION_MODEL=ollama:phi3.5:latest
DR_COMPRESSION_MODEL=ollama:phi3.5:latest

# Research Workflow Configuration
DR_MAX_ITERATIONS=4                  # Number of research iterations (2-10)
DR_MAX_CONCURRENT=2                  # Concurrent operations (1-4)
DR_MAX_TOOL_CALLS=8                  # Maximum tool calls per iteration
DR_ALLOW_CLARIFICATION=false         # Allow interactive clarifications

# Search API Configuration
# Options: tavily, serper, none (for local-only analysis)
DR_SEARCH_API=none

# Model Parameters
DR_MAX_TOKENS=4096                   # Maximum tokens per request
DR_TEMPERATURE=0.1                   # Temperature for analysis (0.0-1.0)

# === API Keys ===
# Cloud LLM Providers (Optional - only needed for cloud models)
# OPENAI_API_KEY=your_openai_api_key_here
# ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Search APIs (Optional - only needed for web search)
# TAVILY_API_KEY=your_tavily_api_key_here
# SERPER_API_KEY=your_serper_api_key_here

# === Stoma Core Configuration ===
# NLP Analysis
ENABLE_NLP=true

# Database Configuration
# PostgreSQL (Primary)
DATABASE_URL=postgresql://localhost:5433/stoma
DB_HOST=localhost
DB_PORT=5433
DB_NAME=stoma
DB_USER=cordlesssteve
# DB_PASSWORD=your_password_here  # If using password authentication

# SQLite (Fallback - no configuration needed)
# SQLITE_DB_PATH=./data/stoma.db

# Redis Configuration (Optional - for caching)
# REDIS_HOST=localhost
# REDIS_PORT=6379
# REDIS_PASSWORD=your_redis_password_here

# === Collection Sources Configuration ===
# ArXiv
ARXIV_MAX_RESULTS=10
ARXIV_DELAY=3  # Seconds between requests

# GitHub
# GITHUB_TOKEN=your_github_token_here
GITHUB_MAX_REPOS=50

# Reddit
# REDDIT_CLIENT_ID=your_reddit_client_id
# REDDIT_CLIENT_SECRET=your_reddit_client_secret
# REDDIT_USER_AGENT=Stoma/0.1.0

# HackerNews
HN_MAX_ITEMS=50

# SEC EDGAR
SEC_USER_AGENT=YourName your.email@example.com
SEC_DELAY=1  # Seconds between requests (SEC requires 100ms minimum)

# === Web Scraping Configuration ===
SCRAPER_USER_AGENT=Mozilla/5.0 (compatible; Stoma/0.1.0)
SCRAPER_TIMEOUT=30
SCRAPER_MAX_RETRIES=3
SCRAPER_RESPECT_ROBOTS_TXT=true
SCRAPER_RATE_LIMIT=1  # Requests per second

# === Content Enrichment Configuration ===
ENABLE_WEB_ENRICHMENT=true
ENABLE_PDF_EXTRACTION=true
PDF_EXTRACTION_TIMEOUT=60

# === Report Storage Configuration ===
REPORTS_BASE_DIR=./reports
ENABLE_POSTGRES_REPORTS=true  # Store report metadata in PostgreSQL
ENABLE_SQLITE_REPORTS=true    # Fallback to SQLite if PostgreSQL unavailable

# === Logging Configuration ===
LOG_LEVEL=INFO  # DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_FILE=./logs/stoma.log
LOG_MAX_BYTES=10485760  # 10MB
LOG_BACKUP_COUNT=5

# === Performance Configuration ===
MAX_WORKERS=4  # Number of concurrent workers
BATCH_SIZE=100  # Items per batch
CACHE_TTL=3600  # Cache time-to-live in seconds

# === Scheduler Configuration ===
ENABLE_SCHEDULER=false
SCHEDULER_INTERVAL=3600  # Seconds between scheduled runs

# === Development Configuration ===
DEBUG=false
TESTING=false
MOCK_EXTERNAL_APIS=false