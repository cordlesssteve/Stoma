{
  "timestamp": "2025-09-25T17:38:09.118488",
  "query": "AI agents",
  "model": "gemma2:2b",
  "papers_count": 5,
  "analysis": {
    "document_id": "collection_only_20250925_173809",
    "title": "Paper Collection: AI agents",
    "research_quality_score": 0,
    "novel_contributions": [
      "LLM analysis not available"
    ],
    "technical_innovations": [
      "LLM analysis not available"
    ],
    "business_implications": [
      "LLM analysis not available"
    ],
    "raw_analysis": "Collection only - LLM analysis skipped due to service unavailability",
    "tokens_used": 0
  },
  "papers": [
    {
      "title": "EmbeddingGemma: Powerful and Lightweight Text Representations",
      "abstract": "We introduce EmbeddingGemma, a new lightweight, open text embedding model\nbased on the Gemma 3 language model family. Our innovative training recipe\nstrategically captures knowledge from larger models via encoder-decoder\ninitialization and geometric embedding distillation. We improve model\nrobustness and expressiveness with a spread-out regularizer, and ensure\ngeneralizability by merging checkpoints from varied, optimized mixtures.\nEvaluated on the Massive Text Embedding Benchmark (MTEB) across multilingual,\nEnglish, and code domains, EmbeddingGemma (300M) achieves state-of-the-art\nresults. Notably, it outperforms prior top models, both proprietary and open,\nwith fewer than 500M parameters, and provides performance comparable to models\ndouble its size, offering an exceptional performance-to-cost ratio. Remarkably,\nthis lead persists when quantizing model weights or truncating embedding\noutputs. This makes EmbeddingGemma particularly well-suited for low-latency and\nhigh-throughput use cases such as on-device applications. We provide ablation\nstudies exploring our key design choices. We release EmbeddingGemma to the\ncommunity to promote further research.",
      "authors": [
        "Henrique Schechter Vera",
        "Sahil Dua",
        "Biao Zhang",
        "Daniel Salz",
        "Ryan Mullins",
        "Sindhu Raghuram Panyam",
        "Sara Smoot",
        "Iftekhar Naim",
        "Joe Zou",
        "Feiyang Chen",
        "Daniel Cer",
        "Alice Lisak",
        "Min Choi",
        "Lucas Gonzalez",
        "Omar Sanseviero",
        "Glenn Cameron",
        "Ian Ballantyne",
        "Kat Black",
        "Kaifeng Chen",
        "Weiyi Wang",
        "Zhe Li",
        "Gus Martins",
        "Jinhyuk Lee",
        "Mark Sherwood",
        "Juyeong Ji",
        "Renjie Wu",
        "Jingxiao Zheng",
        "Jyotinder Singh",
        "Abheesht Sharma",
        "Divya Sreepat",
        "Aashi Jain",
        "Adham Elarabawy",
        "AJ Co",
        "Andreas Doumanoglou",
        "Babak Samari",
        "Ben Hora",
        "Brian Potetz",
        "Dahun Kim",
        "Enrique Alfonseca",
        "Fedor Moiseev",
        "Feng Han",
        "Frank Palma Gomez",
        "Gustavo Hern\u00e1ndez \u00c1brego",
        "Hesen Zhang",
        "Hui Hui",
        "Jay Han",
        "Karan Gill",
        "Ke Chen",
        "Koert Chen",
        "Madhuri Shanbhogue",
        "Michael Boratko",
        "Paul Suganthan",
        "Sai Meher Karthik Duddu",
        "Sandeep Mariserla",
        "Setareh Ariafar",
        "Shanfeng Zhang",
        "Shijie Zhang",
        "Simon Baumgartner",
        "Sonam Goenka",
        "Steve Qiu",
        "Tanmaya Dabral",
        "Trevor Walker",
        "Vikram Rao",
        "Waleed Khawaja",
        "Wenlei Zhou",
        "Xiaoqi Ren",
        "Ye Xia",
        "Yichang Chen",
        "Yi-Ting Chen",
        "Zhe Dong",
        "Zhongli Ding",
        "Francesco Visin",
        "Ga\u00ebl Liu",
        "Jiageng Zhang",
        "Kathleen Kenealy",
        "Michelle Casbon",
        "Ravin Kumar",
        "Thomas Mesnard",
        "Zach Gleicher",
        "Cormac Brick",
        "Olivier Lacombe",
        "Adam Roberts",
        "Yunhsuan Sung",
        "Raphael Hoffmann",
        "Tris Warkentin",
        "Armand Joulin",
        "Tom Duerig",
        "Mojtaba Seyedhosseini"
      ],
      "published": "2025-09-24T17:56:51Z",
      "url": "http://arxiv.org/abs/2509.20354v1",
      "arxiv_id": "2509.20354v1"
    },
    {
      "title": "Developer Productivity With and Without GitHub Copilot: A Longitudinal\n  Mixed-Methods Case Study",
      "abstract": "This study investigates the real-world impact of the generative AI (GenAI)\ntool GitHub Copilot on developer activity and perceived productivity. We\nconducted a mixed-methods case study in NAV IT, a large public sector agile\norganization. We analyzed 26,317 unique non-merge commits from 703 of NAV IT's\nGitHub repositories over a two-year period, focusing on commit-based activity\nmetrics from 25 Copilot users and 14 non-users. The analysis was complemented\nby survey responses on their roles and perceived productivity, as well as 13\ninterviews. Our analysis of activity metrics revealed that individuals who used\nCopilot were consistently more active than non-users, even prior to Copilot's\nintroduction. We did not find any statistically significant changes in\ncommit-based activity for Copilot users after they adopted the tool, although\nminor increases were observed. This suggests a discrepancy between changes in\ncommit-based metrics and the subjective experience of productivity.",
      "authors": [
        "Viktoria Stray",
        "Elias Goldmann Brandtz\u00e6g",
        "Viggo Tellefsen Wivestad",
        "Astri Barbala",
        "Nils Brede Moe"
      ],
      "published": "2025-09-24T17:55:56Z",
      "url": "http://arxiv.org/abs/2509.20353v1",
      "arxiv_id": "2509.20353v1"
    },
    {
      "title": "Statistical Inference Leveraging Synthetic Data with Distribution-Free\n  Guarantees",
      "abstract": "The rapid proliferation of high-quality synthetic data -- generated by\nadvanced AI models or collected as auxiliary data from related tasks --\npresents both opportunities and challenges for statistical inference. This\npaper introduces a GEneral Synthetic-Powered Inference (GESPI) framework that\nwraps around any statistical inference procedure to safely enhance sample\nefficiency by combining synthetic and real data. Our framework leverages\nhigh-quality synthetic data to boost statistical power, yet adaptively defaults\nto the standard inference method using only real data when synthetic data is of\nlow quality. The error of our method remains below a user-specified bound\nwithout any distributional assumptions on the synthetic data, and decreases as\nthe quality of the synthetic data improves. This flexibility enables seamless\nintegration with conformal prediction, risk control, hypothesis testing, and\nmultiple testing procedures, all without modifying the base inference method.\nWe demonstrate the benefits of our method on challenging tasks with limited\nlabeled data, including AlphaFold protein structure prediction, and comparing\nlarge reasoning models on complex math problems.",
      "authors": [
        "Meshi Bashari",
        "Yonghoon Lee",
        "Roy Maor Lotan",
        "Edgar Dobriban",
        "Yaniv Romano"
      ],
      "published": "2025-09-24T17:37:14Z",
      "url": "http://arxiv.org/abs/2509.20345v1",
      "arxiv_id": "2509.20345v1"
    },
    {
      "title": "Efficient Encoder-Free Pose Conditioning and Pose Control for Virtual\n  Try-On",
      "abstract": "As online shopping continues to grow, the demand for Virtual Try-On (VTON)\ntechnology has surged, allowing customers to visualize products on themselves\nby overlaying product images onto their own photos. An essential yet\nchallenging condition for effective VTON is pose control, which ensures\naccurate alignment of products with the user's body while supporting diverse\norientations for a more immersive experience. However, incorporating pose\nconditions into VTON models presents several challenges, including selecting\nthe optimal pose representation, integrating poses without additional\nparameters, and balancing pose preservation with flexible pose control.\n  In this work, we build upon a baseline VTON model that concatenates the\nreference image condition without external encoder, control network, or complex\nattention layers. We investigate methods to incorporate pose control into this\npure concatenation paradigm by spatially concatenating pose data, comparing\nperformance using pose maps and skeletons, without adding any additional\nparameters or module to the baseline model. Our experiments reveal that pose\nstitching with pose maps yields the best results, enhancing both pose\npreservation and output realism. Additionally, we introduce a mixed-mask\ntraining strategy using fine-grained and bounding box masks, allowing the model\nto support flexible product integration across varied poses and conditions.",
      "authors": [
        "Qi Li",
        "Shuwen Qiu",
        "Julien Han",
        "Xingzi Xu",
        "Mehmet Saygin Seyfioglu",
        "Kee Kiat Koo",
        "Karim Bouyarmane"
      ],
      "published": "2025-09-24T17:35:23Z",
      "url": "http://arxiv.org/abs/2509.20343v1",
      "arxiv_id": "2509.20343v1"
    },
    {
      "title": "Adaptive Event-Triggered Policy Gradient for Multi-Agent Reinforcement\n  Learning",
      "abstract": "Conventional multi-agent reinforcement learning (MARL) methods rely on\ntime-triggered execution, where agents sample and communicate actions at fixed\nintervals. This approach is often computationally expensive and\ncommunication-intensive. To address this limitation, we propose ET-MAPG\n(Event-Triggered Multi-Agent Policy Gradient reinforcement learning), a\nframework that jointly learns an agent's control policy and its\nevent-triggering policy. Unlike prior work that decouples these mechanisms,\nET-MAPG integrates them into a unified learning process, enabling agents to\nlearn not only what action to take but also when to execute it. For scenarios\nwith inter-agent communication, we introduce AET-MAPG, an attention-based\nvariant that leverages a self-attention mechanism to learn selective\ncommunication patterns. AET-MAPG empowers agents to determine not only when to\ntrigger an action but also with whom to communicate and what information to\nexchange, thereby optimizing coordination. Both methods can be integrated with\nany policy gradient MARL algorithm. Extensive experiments across diverse MARL\nbenchmarks demonstrate that our approaches achieve performance comparable to\nstate-of-the-art, time-triggered baselines while significantly reducing both\ncomputational load and communication overhead.",
      "authors": [
        "Umer Siddique",
        "Abhinav Sinha",
        "Yongcan Cao"
      ],
      "published": "2025-09-24T17:29:56Z",
      "url": "http://arxiv.org/abs/2509.20338v1",
      "arxiv_id": "2509.20338v1"
    }
  ],
  "tokens_used": 0
}