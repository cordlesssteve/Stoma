#!/usr/bin/env python3
"""
Test Advanced Model Management System

This script tests the advanced model manager integration with batch processing
and cross-paper synthesis, demonstrating intelligent model selection.
"""

import asyncio
import json
import logging
from datetime import datetime
from pathlib import Path

from advanced_model_manager import AdvancedModelManager
from batch_processor import BatchProcessor, BatchProcessingConfig
from cross_paper_synthesizer import CrossPaperSynthesizer, ResearchPaper

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


async def test_model_selection():
    """Test intelligent model selection for different tasks."""
    print("üß† Testing Intelligent Model Selection")
    print("=" * 40)

    manager = AdvancedModelManager()

    # Test different task types
    task_types = [
        "general",
        "code_analysis",
        "research_analysis",
        "synthesis",
        "technical_papers"
    ]

    for task_type in task_types:
        selected_model = manager.select_model_for_task(
            task_type=task_type,
            content_length=5000,
            performance_preference="balanced"
        )
        print(f"üìä {task_type:20} ‚Üí {selected_model}")

        # Get model info
        model_info = manager.get_model_info(selected_model)
        if model_info:
            print(f"     Performance: {model_info.performance_tier}, Specialty: {model_info.specialty}")

    print("\n‚úÖ Model selection test complete!")


async def test_batch_processor_with_advanced_models():
    """Test batch processor with intelligent model selection."""
    print("\nüîÑ Testing Batch Processor with Advanced Models")
    print("=" * 50)

    # Create test configuration
    config = BatchProcessingConfig(
        max_concurrent_jobs=1,
        storage_path="./reports/test_advanced_models",
        max_papers_per_job=3  # Small for testing
    )

    # Initialize processor
    processor = BatchProcessor(config)

    print("‚úÖ Batch processor initialized with advanced model manager")

    # Test system analysis
    processor.model_manager.print_system_analysis()

    # Schedule a test job
    job_id = processor.scheduler.schedule_arxiv_analysis(
        query="machine learning optimization",
        max_papers=3,
        model="gemma2:2b",  # This will be automatically optimized
        priority=1
    )

    print(f"\nüìÖ Scheduled test job: {job_id}")

    # Run one iteration to see model selection in action
    print("\nüîÑ Running batch processor (1 iteration)...")
    try:
        await processor.run_batch_processor(max_iterations=1)
    except Exception as e:
        print(f"Expected behavior - limited iterations: {e}")

    # Check if model selection worked
    status = processor.scheduler.get_job_status()
    print(f"\nCompleted jobs: {status['completed_jobs']}")

    print("\n‚úÖ Batch processor with advanced models test complete!")


async def test_cross_paper_synthesis_models():
    """Test cross-paper synthesis with different models."""
    print("\nüî¨ Testing Cross-Paper Synthesis with Model Selection")
    print("=" * 50)

    manager = AdvancedModelManager()

    # Get available models
    available_models = []
    try:
        import subprocess
        result = subprocess.run(['ollama', 'list'], capture_output=True, text=True, timeout=10)
        if result.returncode == 0:
            lines = result.stdout.strip().split('\n')[1:]  # Skip header
            available_models = [line.split()[0] for line in lines if line.strip()]
    except Exception:
        available_models = ["qwen2.5-coder:3b"]  # Fallback

    if not available_models:
        print("‚ùå No models available for testing")
        return

    print(f"Available models: {available_models}")

    # Select best model for synthesis
    optimal_model = manager.select_model_for_task(
        task_type="synthesis",
        content_length=10000,
        performance_preference="quality"
    )

    print(f"üéØ Selected optimal model for synthesis: {optimal_model}")

    # Test synthesis with selected model
    test_papers = [
        ResearchPaper(
            paper_id="test_1",
            title="Efficient Neural Architecture Search with Differentiable Architecture",
            abstract="We propose a differentiable approach to neural architecture search that reduces search time from thousands to single-digit GPU days.",
            authors=["Hanxiao Liu", "Karen Simonyan"],
            published_date="2018-06-18",
            arxiv_id="1806.09055",
            url="https://arxiv.org/abs/1806.09055",
            domain="machine_learning",
            keywords=["nas", "neural architecture", "optimization"],
            analysis_result={
                "technical_innovations": ["differentiable architecture search", "continuous relaxation", "gradient-based optimization"],
                "business_implications": ["reduced computational cost", "faster model development"],
                "research_quality_score": 8
            }
        ),
        ResearchPaper(
            paper_id="test_2",
            title="EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
            abstract="We systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance.",
            authors=["Mingxing Tan", "Quoc V. Le"],
            published_date="2019-05-28",
            arxiv_id="1905.11946",
            url="https://arxiv.org/abs/1905.11946",
            domain="computer_vision",
            keywords=["model scaling", "efficientnet", "cnn"],
            analysis_result={
                "technical_innovations": ["compound scaling", "balanced scaling dimensions", "efficient architecture"],
                "business_implications": ["mobile deployment", "reduced inference cost"],
                "research_quality_score": 9
            }
        )
    ]

    # Run synthesis
    print(f"\nüîÑ Running synthesis with {optimal_model}...")
    synthesizer = CrossPaperSynthesizer(model=optimal_model)

    try:
        result = await synthesizer.synthesize_papers(test_papers)

        print(f"‚úÖ Synthesis completed!")
        print(f"   Quality Score: {result.synthesis_quality_score:.2f}")
        print(f"   Trends Found: {len(result.cross_domain_trends)}")
        print(f"   Common Techniques: {len(result.common_techniques)}")

        if result.cross_domain_trends:
            print("\nüî• Top Trend:")
            trend = result.cross_domain_trends[0]
            print(f"   {trend.trend_name} (confidence: {trend.confidence_score:.2f})")

    except Exception as e:
        print(f"‚ùå Synthesis test failed: {e}")

    print("\n‚úÖ Cross-paper synthesis with models test complete!")


def test_model_compatibility():
    """Test model compatibility analysis."""
    print("\nüñ•Ô∏è  Testing Model Compatibility Analysis")
    print("=" * 40)

    manager = AdvancedModelManager()

    # Print system analysis
    manager.print_system_analysis()

    # Get compatibility matrix
    compatibility = manager.get_model_compatibility_matrix()

    print("\nüìä Model Performance Analysis:")

    # Group models by compatibility
    can_run = []
    cannot_run = []

    for model_name, info in compatibility.items():
        if info["can_run_cpu"] or info["can_run_gpu"]:
            can_run.append(model_name)
        else:
            cannot_run.append(model_name)

    print(f"\n‚úÖ Compatible models ({len(can_run)}):")
    for model in can_run:
        model_info = compatibility[model]["model_info"]
        print(f"   - {model} ({model_info.performance_tier}, {model_info.size_gb:.1f}GB)")

    if cannot_run:
        print(f"\n‚ùå Incompatible models ({len(cannot_run)}):")
        for model in cannot_run:
            model_info = compatibility[model]["model_info"]
            print(f"   - {model} ({model_info.performance_tier}, {model_info.size_gb:.1f}GB)")

    print("\n‚úÖ Compatibility analysis complete!")


async def test_performance_preferences():
    """Test different performance preferences for model selection."""
    print("\n‚ö° Testing Performance Preferences")
    print("=" * 35)

    manager = AdvancedModelManager()

    preferences = ["speed", "balanced", "quality"]
    task_type = "research_analysis"
    content_length = 8000

    print(f"Task: {task_type}, Content length: {content_length}")

    for preference in preferences:
        selected_model = manager.select_model_for_task(
            task_type=task_type,
            content_length=content_length,
            performance_preference=preference
        )

        model_info = manager.get_model_info(selected_model)
        if model_info:
            print(f"{preference:10} ‚Üí {selected_model:20} ({model_info.estimated_speed}, {model_info.size_gb:.1f}GB)")
        else:
            print(f"{preference:10} ‚Üí {selected_model:20} (info not available)")

    print("\n‚úÖ Performance preference test complete!")


async def main():
    """Run all advanced model tests."""
    print("üöÄ Stoma Advanced Model Management Tests")
    print("=" * 50)

    try:
        # Test 1: Model selection
        await test_model_selection()

        # Test 2: System compatibility
        test_model_compatibility()

        # Test 3: Performance preferences
        await test_performance_preferences()

        # Test 4: Cross-paper synthesis with models
        await test_cross_paper_synthesis_models()

        # Test 5: Batch processor integration
        await test_batch_processor_with_advanced_models()

        print("\nüéâ All advanced model tests completed!")
        print("The advanced model management system is ready for production use.")

    except Exception as e:
        print(f"\n‚ùå Test failed with error: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())