#!/usr/bin/env python3
"""
Test script for lightweight Ollama models (Phi-3.5, small Qwen, etc.).

Optimized for memory efficiency and faster analysis while still providing
better insights than basic keyword extraction.
"""

import asyncio
import logging
from typing import Dict, Any, List
import json

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Test if we can import the LLM modules
try:
    from stoma.analysis.llm_analyzer import LLMAnalyzer, LLMAnalysisService
    LLM_AVAILABLE = True
except ImportError as e:
    logger.error(f"LLM modules not available: {e}")
    LLM_AVAILABLE = False


# Lightweight models optimized for memory and speed
LIGHTWEIGHT_MODELS = {
    "phi3.5:mini": {
        "description": "Microsoft Phi-3.5-mini (3.8B params)",
        "use_case": "Fast, memory-efficient analysis with good quality",
        "memory_req": "~2-3GB VRAM",
        "install": "ollama pull phi3.5"
    },
    "phi3:mini": {
        "description": "Microsoft Phi-3-mini (3.8B params)",
        "use_case": "Older but stable lightweight model",
        "memory_req": "~2-3GB VRAM",
        "install": "ollama pull phi3"
    },
    "qwen2.5:3b": {
        "description": "Qwen2.5-3B-Instruct",
        "use_case": "Excellent small model for research analysis",
        "memory_req": "~3GB VRAM",
        "install": "ollama pull qwen2.5:3b"
    },
    "qwen2.5:7b": {
        "description": "Qwen2.5-7B-Instruct",
        "use_case": "Balanced performance and memory usage",
        "memory_req": "~5-6GB VRAM",
        "install": "ollama pull qwen2.5:7b"
    },
    "gemma2:2b": {
        "description": "Google Gemma-2-2B",
        "use_case": "Ultra-lightweight, good for quick summaries",
        "memory_req": "~2GB VRAM",
        "install": "ollama pull gemma2:2b"
    },
    "mistral:7b": {
        "description": "Mistral-7B-Instruct",
        "use_case": "Good balance of speed and quality",
        "memory_req": "~5-6GB VRAM",
        "install": "ollama pull mistral"
    },
    "llama3.2:3b": {
        "description": "Meta Llama-3.2-3B",
        "use_case": "Latest small Llama model, excellent quality",
        "memory_req": "~3GB VRAM",
        "install": "ollama pull llama3.2:3b"
    }
}


class LightweightAnalyzer:
    """Specialized analyzer for lightweight models with optimized prompts."""
    
    def __init__(self, provider="ollama", model="phi3.5"):
        """Initialize with a lightweight model."""
        self.provider = provider
        self.model = model
        self.analyzer = None
        if LLM_AVAILABLE:
            self.analyzer = LLMAnalyzer(
                provider=provider,
                model=model,
                max_tokens=800,  # Reduced for faster responses
                temperature=0.1   # Lower temperature for consistency
            )
    
    async def analyze_paper_lightweight(self, text: str, title: str) -> Dict[str, Any]:
        """
        Perform lightweight but effective analysis optimized for small models.
        
        Focus on key insights rather than exhaustive analysis.
        """
        if not self.analyzer:
            return self._fallback_analysis(text, title)
        
        # Simplified prompt for lightweight models
        prompt = f"""Analyze this research paper concisely. Focus on the most important points.

Title: {title}

Content: {text[:2000]}  # Limit content for faster processing

Provide brief answers (1-2 sentences each):
1. Main contribution: What is the key innovation?
2. Technical approach: How does it work?
3. Results: What improvement was achieved?
4. Significance: Why does this matter?
5. Applications: Where can this be used?

Format as JSON with keys: contribution, approach, results, significance, applications"""

        try:
            # Use the existing _call_llm method
            result = await self.analyzer._call_llm(prompt)
            
            # Parse the response
            try:
                analysis = json.loads(result)
            except:
                # Fallback to text parsing if JSON fails
                analysis = self._parse_text_response(result)
            
            return {
                "model": self.model,
                "analysis": analysis,
                "quality_score": self._calculate_quality_score(analysis),
                "summary": self._generate_summary(analysis)
            }
            
        except Exception as e:
            logger.error(f"Analysis failed: {e}")
            return self._fallback_analysis(text, title)
    
    def _parse_text_response(self, response: str) -> Dict[str, str]:
        """Parse text response if JSON parsing fails."""
        analysis = {
            "contribution": "Unable to parse",
            "approach": "Unable to parse",
            "results": "Unable to parse",
            "significance": "Unable to parse",
            "applications": "Unable to parse"
        }
        
        # Simple text parsing
        lines = response.split('\n')
        for line in lines:
            line_lower = line.lower()
            if 'contribution' in line_lower or 'innovation' in line_lower:
                analysis["contribution"] = line.split(':', 1)[-1].strip()
            elif 'approach' in line_lower or 'method' in line_lower:
                analysis["approach"] = line.split(':', 1)[-1].strip()
            elif 'result' in line_lower:
                analysis["results"] = line.split(':', 1)[-1].strip()
            elif 'significance' in line_lower or 'matter' in line_lower:
                analysis["significance"] = line.split(':', 1)[-1].strip()
            elif 'application' in line_lower or 'use' in line_lower:
                analysis["applications"] = line.split(':', 1)[-1].strip()
        
        return analysis
    
    def _calculate_quality_score(self, analysis: Dict[str, str]) -> float:
        """Calculate a simple quality score based on analysis completeness."""
        score = 0.0
        max_score = 5.0
        
        for key, value in analysis.items():
            if value and len(value) > 20 and value != "Unable to parse":
                score += 1.0
        
        return (score / max_score) * 10  # Scale to 0-10
    
    def _generate_summary(self, analysis: Dict[str, str]) -> str:
        """Generate a brief summary from the analysis."""
        parts = []
        
        if analysis.get("contribution") and analysis["contribution"] != "Unable to parse":
            parts.append(f"Key contribution: {analysis['contribution']}")
        
        if analysis.get("results") and analysis["results"] != "Unable to parse":
            parts.append(f"Results: {analysis['results']}")
        
        if analysis.get("significance") and analysis["significance"] != "Unable to parse":
            parts.append(f"Impact: {analysis['significance']}")
        
        return " ".join(parts) if parts else "Analysis incomplete"
    
    def _fallback_analysis(self, text: str, title: str) -> Dict[str, Any]:
        """Fallback analysis when LLM is not available."""
        # Better than pure keyword extraction but not as good as LLM
        words = text.lower().split()
        
        # Look for research indicators
        indicators = {
            "novel": ["novel", "new", "first", "innovative", "breakthrough"],
            "improvement": ["improve", "better", "enhance", "optimize", "efficient"],
            "method": ["method", "approach", "algorithm", "technique", "framework"],
            "results": ["result", "achieve", "demonstrate", "show", "performance"]
        }
        
        found = {}
        for category, keywords in indicators.items():
            found[category] = any(word in words for word in keywords)
        
        return {
            "model": "fallback",
            "analysis": {
                "contribution": "Novel approach detected" if found.get("novel") else "Standard approach",
                "approach": "New method proposed" if found.get("method") else "Existing methods used",
                "results": "Performance improvements shown" if found.get("improvement") else "Results reported",
                "significance": "High" if found.get("novel") and found.get("improvement") else "Moderate",
                "applications": "Research applications identified"
            },
            "quality_score": sum(found.values()) * 2.5,
            "summary": f"Paper '{title}' analyzed using fallback method"
        }


async def check_ollama_lightweight():
    """Check if Ollama is running and suggest lightweight models."""
    print("üîç Checking Ollama for Lightweight Models")
    print("=" * 50)
    
    try:
        import aiohttp
        async with aiohttp.ClientSession() as session:
            async with session.get("http://localhost:11434/api/tags") as response:
                if response.status == 200:
                    result = await response.json()
                    models = [model['name'] for model in result.get('models', [])]
                    
                    print("‚úÖ Ollama is running!")
                    print(f"üì¶ Available models: {len(models)}")
                    
                    # Check for lightweight models
                    lightweight_found = []
                    for model in models:
                        for lightweight in LIGHTWEIGHT_MODELS:
                            if lightweight.split(':')[0] in model.lower():
                                lightweight_found.append(model)
                                break
                    
                    if lightweight_found:
                        print(f"\n‚ú® Lightweight models found: {lightweight_found}")
                    else:
                        print("\n‚ö†Ô∏è  No lightweight models installed yet")
                        print("\nüí° Quick setup for lightweight analysis:")
                        print("   ollama pull phi3.5      # Microsoft Phi-3.5 (2-3GB)")
                        print("   ollama pull qwen2.5:3b  # Qwen 2.5 3B (3GB)")
                        print("   ollama pull gemma2:2b   # Google Gemma 2B (2GB)")
                    
                    return True, models
                    
    except Exception as e:
        print(f"‚ùå Ollama not available: {e}")
        print("\nüìù Installation Instructions:")
        print("1. Install Ollama:")
        print("   ‚Ä¢ Linux/Mac: curl -fsSL https://ollama.com/install.sh | sh")
        print("   ‚Ä¢ Windows: Download from https://ollama.com/download")
        print("\n2. Start Ollama service:")
        print("   ‚Ä¢ ollama serve")
        print("\n3. Pull a lightweight model:")
        print("   ‚Ä¢ ollama pull phi3.5")
        print("   ‚Ä¢ ollama pull qwen2.5:3b")
        return False, []


async def test_lightweight_analysis():
    """Test lightweight model analysis with a sample paper."""
    print("\nüß™ Testing Lightweight Analysis")
    print("=" * 50)
    
    # Sample paper for testing
    test_paper = {
        "title": "Efficient Fine-Tuning with Dynamic LoRA Adapters",
        "content": """
        We introduce a novel approach to parameter-efficient fine-tuning using dynamic Low-Rank Adaptation (LoRA) adapters. Our method automatically adjusts the rank of LoRA modules based on layer importance, reducing parameters by 90% while maintaining 98% of full fine-tuning performance. 
        
        Key innovation: Dynamic rank selection algorithm that analyzes gradient flow to determine optimal rank values per layer. This eliminates the need for manual hyperparameter tuning.
        
        Results on GLUE benchmark show our approach outperforms static LoRA by 5% while using 30% fewer parameters. The method is particularly effective for resource-constrained environments.
        
        Applications include mobile deployment, edge computing, and rapid model adaptation with limited compute resources.
        """
    }
    
    # Test with lightweight analyzer
    analyzer = LightweightAnalyzer(provider="ollama", model="phi3.5")
    
    print(f"üìÑ Analyzing: {test_paper['title']}")
    print("ü§ñ Using: Lightweight Ollama model (phi3.5 or fallback)")
    
    result = await analyzer.analyze_paper_lightweight(
        text=test_paper["content"],
        title=test_paper["title"]
    )
    
    print(f"\n‚úÖ Analysis Complete!")
    print(f"   Model: {result['model']}")
    print(f"   Quality Score: {result['quality_score']:.1f}/10")
    print(f"\nüìä Analysis Results:")
    
    analysis = result['analysis']
    for key, value in analysis.items():
        print(f"   ‚Ä¢ {key.capitalize()}: {value}")
    
    print(f"\nüìù Summary:")
    print(f"   {result['summary']}")
    
    # Compare with keyword extraction
    print("\nüîÑ Comparison with Basic Keyword Extraction:")
    keywords = ["lora", "adapter", "efficient", "parameter", "fine-tuning"]
    print(f"   Keywords: {', '.join(keywords)}")
    print(f"   ‚ùå No semantic understanding")
    print(f"   ‚ùå No contribution identification")
    print(f"   ‚ùå No significance assessment")
    
    print("\n‚ú® Lightweight LLM Advantages:")
    print(f"   ‚úÖ Identifies main contribution")
    print(f"   ‚úÖ Understands technical approach")
    print(f"   ‚úÖ Assesses results and impact")
    print(f"   ‚úÖ Suggests applications")
    
    return result


async def show_model_recommendations():
    """Show recommended lightweight models."""
    print("\nüéØ Recommended Lightweight Models")
    print("=" * 50)
    print("For memory-efficient research analysis:\n")
    
    for model_name, info in LIGHTWEIGHT_MODELS.items():
        print(f"üìã **{model_name}**")
        print(f"   {info['description']}")
        print(f"   Use: {info['use_case']}")
        print(f"   Memory: {info['memory_req']}")
        print(f"   Install: {info['install']}")
        print()
    
    print("üí° **Recommendations for Your Use Case:**")
    print("1. Start with phi3.5 or qwen2.5:3b (best quality/size ratio)")
    print("2. Use gemma2:2b for ultra-fast summaries")
    print("3. Scale to qwen2.5:7b or mistral:7b if you have 6GB+ VRAM")
    print("\n‚ö° These models provide 10-100x better analysis than keyword extraction")
    print("   while using minimal memory (2-6GB vs 40GB+ for large models)")


async def main():
    """Run lightweight Ollama tests."""
    print("üöÄ Stoma Lightweight LLM Testing")
    print("=" * 50)
    print("Testing memory-efficient models for better-than-keyword analysis\n")
    
    # Check Ollama status
    ollama_running, models = await check_ollama_lightweight()
    
    # Show recommendations
    await show_model_recommendations()
    
    # Test lightweight analysis
    if LLM_AVAILABLE:
        await test_lightweight_analysis()
    else:
        print("\n‚ö†Ô∏è  LLM modules not available for testing")
        print("   But the infrastructure is ready for lightweight models!")
    
    print("\n" + "=" * 50)
    print("üìù Summary: Lightweight models offer excellent analysis quality")
    print("   with minimal resource requirements (2-6GB VRAM)")
    print("   Perfect for continuous analysis without high costs!")


if __name__ == "__main__":
    asyncio.run(main())